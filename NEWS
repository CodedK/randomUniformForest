Changes in 1.1.0

1- New method used to choose cut-points lead to loss of accuracy for low values of 'mtry' and in the unsupervised case. 
Cut-points choice has been changed in order to separate classification, unsupervised mode and regression. 
In Regression and unsupervised learning, cut-points are chosen using the whole support of each candidate variable, 
as in initial release and prior to the 1.0.9 one. In Classification, the support is now reduced to two random points 
chosen among a small number. This lead to more diversity in trees. 

2 - Categorical variables were not correctly matched when one was using 'categoricalvariablesidx' option.

3 - Better handling of categorical variables when assessing partial dependencies.

Changes in 1.0.9 - 1.1.x :

1 - Cut-points of each node are now chosen between two random points of each candidate variable. Previously, the whole support was chosen.
This update on the optimization criterion leads to faster computation while not changing accuracy. 

2 - Introduction of unsupervised learning using the following chain commands paradigm:
pre-processing (following Breiman's ideas) + randomUniformForest proximities matrix +  Multidimensional scaling + 
gap statistics (for estimating the number of clusters) + kmeans (for identifying every observation)
Unsupervised learning introduces then many concepts that still need to be properly referenced. 
Note that implementation for large datasets is currently not integrated.

3 - model.stats() and generic.cv()  are now documented and bugs fixed.
model.stats() function displays many informations (confusion matrix, ROC curves, residuals,...) for any vector
of predictions and responses. generic.cv() function proceeds to a k-fold cross-validation for any algorithm (with however some manual code).

4 - Better matching of NA Values and comments to allow the right choice of imputation when predicting data with missing values.

5 - Improvements in speed and accuracy when using one (or both) of the options 'depth' or 'depthcontrol'.
Especially for large files (> 100 000 rows), increase both speed and accuracy when using the rUniformForest.big() function.
Note that the latter splits data in chunks, therefore one can not reach the accuracy of an (or the same) algorithm using the whole sample.

6 - Variable importance bug corrected for rUniformForest.big() function.

7 - Variable importance bug corrected when using formula in randomUniformForest() function.

8 - Improvements in speed, especially for large samples (> 50 000 rows) and high dimension (> 100 variables) in the regression case.
More precisely, large datasets take most of the benefits of optimizations added.

9 - Improvements in the accuracy of variable importance

10 - Minor bugs corrected

Changes in 1.0.7 - 1.0.8 :

1- Full support of categorical variables.
They were formerly matched by the same function than continuous ones, leading to some troubles in Variable importance and a loss of accuracy.
Now, categorical variables used their own engine, from modelling to variable importance and selection.
In addition, one can now use discrete values as categorical, e.g., in order to know if one frequent value in a variable can affect the response.
Note that accuracy might drop in contrast of the default case, for which algorithm considers all variables as continuous ones,

2 - Partial dependence plots.
better match of categorical variables and 3D representation

3 - Quantile regression.
Value of quantile was between 1 and 99 and not 0 and 1 as required.

4 - Variable importance. 
Bugs correction : when prediction Object was present, importance was not correctly computed, leading to wrong interactions between features.
Interactions plot was also inverted (1rst order was 2nd).

5 - Variable importance is now allowed with mtry = 1. As a consequence, one can assess variable importance using purely random forest.

6 - outputperturbationSampling.
It is now allowed to replace all original values of train responses by a random vector sampled from those values (mean and variance)
using gaussian distribution.

7 - Summary.
random Uniform forests summary is now unified and one can also use model.stats( ) function to assess predictions vs responses.

8 - Missing values.
fillNA2.randomUniformForest( ) is now working with large files.

9 - Prediction. 
Prediction is now possible for only one observation.

10- Rebalanced sampling.
Sample of any size or class distribution is allowed with rebalancedsampling option.

11 - Area Under Precision-Recall Curve is available using roc.curve( ) function.

12 - rUniformForest.grow( ) function is now working with outputperturbationSampling enabled in first run before adding more trees.

13 - Various bugs corrected.